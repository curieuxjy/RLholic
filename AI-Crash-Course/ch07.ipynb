{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3381360-9e33-41f4-b58e-c76be85d4c9d",
   "metadata": {},
   "source": [
    "# Q-Learning Pseudocode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0a11d5-99c2-4e6e-acc4-13ff93d7b170",
   "metadata": {},
   "source": [
    "```\n",
    "┌─────────────────────────────┬────────\n",
    "│                             │\n",
    "│    A         B         C    │    D\n",
    "│                             │\n",
    "└─────────┐         \n",
    "          │         │                  │\n",
    "     E    │    F    │    G         H   │\n",
    "          │         │                  │\n",
    "                    └─────────         │\n",
    "│                                      │\n",
    "│    I         J         K         L   │\n",
    "│                                      │\n",
    "└───────────────────           ────────┘\n",
    "\n",
    "```\n",
    "\n",
    "|location|state|\n",
    "|-|-|\n",
    "|A|0|\n",
    "|B|1|\n",
    "|C|2|\n",
    "|D|3|\n",
    "|E|4|\n",
    "|F|5|\n",
    "|G|6|\n",
    "|H|7|\n",
    "|I|8|\n",
    "|J|9|\n",
    "|K|10|\n",
    "|L|11|\n",
    "\n",
    "action은 도착할 수 있는 location의 인덱스.\n",
    "\n",
    "도착할 수 있는 location을 선택할 경우 1, 아니면 0 reward\n",
    "\n",
    "G로 보내고 싶을 때 AI agent에게 주어지는 보상 행렬은 다음과 같다.\n",
    "\n",
    "||A|B|C|D|E|F|G|H|I|J|K|L|\n",
    "|-|-|-|-|-|-|-|-|-|-|-|-|-|\n",
    "|A|0|1|0|0|0|0|0|0|0|0|0|0|\n",
    "|B|1|0|1|0|0|1|0|0|0|0|0|0|\n",
    "|C|0|1|0|0|0|0|1|0|0|0|0|0|\n",
    "|D|0|0|0|0|0|0|0|1|0|0|0|0|\n",
    "|E|0|0|0|0|0|0|0|0|1|0|0|0|\n",
    "|F|0|1|0|0|0|0|0|0|0|1|0|0|\n",
    "|G|0|0|1|0|0|0|1000|1|0|0|0|0|\n",
    "|H|0|0|0|1|0|0|1|0|0|0|0|1|\n",
    "|I|0|0|0|0|1|0|0|0|0|1|0|0|\n",
    "|J|0|0|0|0|0|1|0|0|1|0|1|0|\n",
    "|K|0|0|0|0|0|0|0|0|0|1|0|1|\n",
    "|L|0|0|0|0|0|0|0|1|0|0|1|0|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24aa6688-d3f9-4fd8-b8cf-f0b231cc9d8a",
   "metadata": {},
   "source": [
    "Environment()와 AI() class 구성 한눈에 보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51f6ddf-7c9f-4112-aa24-506fc7f67f64",
   "metadata": {},
   "source": [
    "```python\n",
    "class Environment():\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\tInitialize the Environment\n",
    "\t\n",
    "\tdef get_random_state(self):\n",
    "\t\tReturn a random possible state of the game\n",
    "\t\n",
    "\tdef get_qvalue(self, random_state, action):\n",
    "\t\tReturn the Q-value of this random_state, action couple\t\n",
    "\n",
    "\tdef update(self, action):\n",
    "\t\tUpdate the environment, reach the next state and return the Q-values of this new state\n",
    "\n",
    "\tdef get_reward(self, random_state, action):\n",
    "\t\tReturn the reward obtained by playing this action from this random possible state\n",
    "\t\n",
    "\tdef calculate_TD(self, qvalue, next_state, reward, gamma):\n",
    "\t\tReturn the calculated Temporal Difference using the equation: TD = reward + gamma*max(qvalues_next_state) - qvalue\n",
    "\n",
    "\tdef update_qvalue(self, TD, qvalue, alpha):\n",
    "\t\tUpdate the qvalue specified as argument using the equation: qvalue = qvalue + alpha * TD\n",
    "\n",
    "class AI():\n",
    "\t\n",
    "\tdef __init__(self):\n",
    "\t\tInitialize the AI\n",
    "\n",
    "\tdef play_action(self):\n",
    "\t\tPlay a random action\t\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea83e62-1373-4934-a8f5-584515ba9aa5",
   "metadata": {},
   "source": [
    "만들어진 Environment와 AI를 가지고 Q-learning을 하는 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415e3d87-8d19-45b8-b01d-963bf1b82a2f",
   "metadata": {},
   "source": [
    "```python\t\t\t\t\t\t\n",
    "env = Environment()\n",
    "ai = AI()\n",
    "\n",
    "Initialize gamma\n",
    "Initialize alpha\n",
    "\n",
    "while True:\n",
    "\trandom_state = env.get_random_state()\n",
    "\n",
    "\taction = ai.play_action()\n",
    "\t\n",
    "\tqvalue = env.get_qvalue(random_state, action)\n",
    "\n",
    "\tnext_state = env.update(action)\n",
    "\n",
    "\treward = env.get_reward(random_state, action)\n",
    "\n",
    "\tTD = env.calculate_TD(qvalue, next_state, reward, gamma)\n",
    "\n",
    "\tenv.update_qvalue(TD, qvalue, alpha)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8380f630-d243-4988-ae25-a43af71c38c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
